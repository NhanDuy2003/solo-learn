{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMj13PDD+zCTZmWIkj7j6wX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NhanDuy2003/solo-learn/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcmT5upfozhs"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "import logging\n",
        "import os\n",
        "\n",
        "import hydra\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from omegaconf import DictConfig, OmegaConf\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.strategies.ddp import DDPStrategy\n",
        "from timm.data.mixup import Mixup\n",
        "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
        "\n",
        "from solo.args.linear import parse_cfg\n",
        "from solo.data.classification_dataloader import prepare_data\n",
        "from solo.methods.base import BaseMethod\n",
        "from solo.methods.linear import LinearModel\n",
        "from solo.utils.auto_resumer import AutoResumer\n",
        "from solo.utils.checkpointer import Checkpointer\n",
        "from solo.utils.misc import make_contiguous\n",
        "\n",
        "try:\n",
        "    from solo.data.dali_dataloader import ClassificationDALIDataModule\n",
        "except ImportError:\n",
        "    _dali_avaliable = False\n",
        "else:\n",
        "    _dali_avaliable = True\n",
        "\n",
        "\n",
        "@hydra.main(version_base=\"1.2\")\n",
        "def main(cfg: DictConfig):\n",
        "    # hydra doesn't allow us to add new keys for \"safety\"\n",
        "    # set_struct(..., False) disables this behavior and allows us to add more parameters\n",
        "    # without making the user specify every single thing about the model\n",
        "    OmegaConf.set_struct(cfg, False)\n",
        "    cfg = parse_cfg(cfg)\n",
        "\n",
        "    backbone_model = BaseMethod._BACKBONES[cfg.backbone.name]\n",
        "\n",
        "    # initialize backbone\n",
        "    backbone = backbone_model(method=cfg.pretrain_method, **cfg.backbone.kwargs)\n",
        "    if cfg.backbone.name.startswith(\"resnet\"):\n",
        "        # remove fc layer\n",
        "        backbone.fc = nn.Identity()\n",
        "        cifar = cfg.data.dataset in [\"cifar10\", \"cifar100\"]\n",
        "        if cifar:\n",
        "            backbone.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=2, bias=False)\n",
        "            backbone.maxpool = nn.Identity()\n",
        "\n",
        "    ckpt_path = cfg.pretrained_feature_extractor\n",
        "    assert ckpt_path.endswith(\".ckpt\") or ckpt_path.endswith(\".pth\") or ckpt_path.endswith(\".pt\")\n",
        "\n",
        "    state = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
        "    for k in list(state.keys()):\n",
        "        if \"encoder\" in k:\n",
        "            state[k.replace(\"encoder\", \"backbone\")] = state[k]\n",
        "            logging.warn(\n",
        "                \"You are using an older checkpoint. Use a new one as some issues might arrise.\"\n",
        "            )\n",
        "        if \"backbone\" in k:\n",
        "            state[k.replace(\"backbone.\", \"\")] = state[k]\n",
        "        del state[k]\n",
        "    backbone.load_state_dict(state, strict=False)\n",
        "    logging.info(f\"Loaded {ckpt_path}\")\n",
        "\n",
        "    # check if mixup or cutmix is enabled\n",
        "    mixup_func = None\n",
        "    mixup_active = cfg.mixup > 0 or cfg.cutmix > 0\n",
        "    if mixup_active:\n",
        "        logging.info(\"Mixup activated\")\n",
        "        mixup_func = Mixup(\n",
        "            mixup_alpha=cfg.mixup,\n",
        "            cutmix_alpha=cfg.cutmix,\n",
        "            cutmix_minmax=None,\n",
        "            prob=1.0,\n",
        "            switch_prob=0.5,\n",
        "            mode=\"batch\",\n",
        "            label_smoothing=cfg.label_smoothing,\n",
        "            num_classes=cfg.data.num_classes,\n",
        "        )\n",
        "        # smoothing is handled with mixup label transform\n",
        "        loss_func = SoftTargetCrossEntropy()\n",
        "    elif cfg.label_smoothing > 0:\n",
        "        loss_func = LabelSmoothingCrossEntropy(smoothing=cfg.label_smoothing)\n",
        "    else:\n",
        "        loss_func = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    model = LinearModel(backbone, loss_func=loss_func, mixup_func=mixup_func, cfg=cfg)\n",
        "    make_contiguous(model)\n",
        "    # can provide up to ~20% speed up\n",
        "    if not cfg.performance.disable_channel_last:\n",
        "        model = model.to(memory_format=torch.channels_last)\n",
        "\n",
        "    if cfg.data.format == \"dali\":\n",
        "        val_data_format = \"image_folder\"\n",
        "    else:\n",
        "        val_data_format = cfg.data.format\n",
        "\n",
        "    train_loader, val_loader = prepare_data(\n",
        "        cfg.data.dataset,\n",
        "        train_data_path=cfg.data.train_path,\n",
        "        val_data_path=cfg.data.val_path,\n",
        "        data_format=val_data_format,\n",
        "        batch_size=cfg.optimizer.batch_size,\n",
        "        num_workers=cfg.data.num_workers,\n",
        "        auto_augment=cfg.auto_augment,\n",
        "    )\n",
        "\n",
        "    if cfg.data.format == \"dali\":\n",
        "        assert (\n",
        "            _dali_avaliable\n",
        "        ), \"Dali is not currently avaiable, please install it first with pip3 install .[dali].\"\n",
        "\n",
        "        assert not cfg.auto_augment, \"Auto augmentation is not supported with Dali.\"\n",
        "\n",
        "        dali_datamodule = ClassificationDALIDataModule(\n",
        "            dataset=cfg.data.dataset,\n",
        "            train_data_path=cfg.data.train_path,\n",
        "            val_data_path=cfg.data.val_path,\n",
        "            num_workers=cfg.data.num_workers,\n",
        "            batch_size=cfg.optimizer.batch_size,\n",
        "            data_fraction=cfg.data.fraction,\n",
        "            dali_device=cfg.dali.device,\n",
        "        )\n",
        "\n",
        "        # use normal torchvision dataloader for validation to save memory\n",
        "        dali_datamodule.val_dataloader = lambda: val_loader\n",
        "\n",
        "    # 1.7 will deprecate resume_from_checkpoint, but for the moment\n",
        "    # the argument is the same, but we need to pass it as ckpt_path to trainer.fit\n",
        "    ckpt_path, wandb_run_id = None, None\n",
        "    if cfg.auto_resume.enabled and cfg.resume_from_checkpoint is None:\n",
        "        auto_resumer = AutoResumer(\n",
        "            checkpoint_dir=os.path.join(cfg.checkpoint.dir, \"linear\"),\n",
        "            max_hours=cfg.auto_resume.max_hours,\n",
        "        )\n",
        "        resume_from_checkpoint, wandb_run_id = auto_resumer.find_checkpoint(cfg)\n",
        "        if resume_from_checkpoint is not None:\n",
        "            print(\n",
        "                \"Resuming from previous checkpoint that matches specifications:\",\n",
        "                f\"'{resume_from_checkpoint}'\",\n",
        "            )\n",
        "            ckpt_path = resume_from_checkpoint\n",
        "    elif cfg.resume_from_checkpoint is not None:\n",
        "        ckpt_path = cfg.resume_from_checkpoint\n",
        "        del cfg.resume_from_checkpoint\n",
        "\n",
        "    callbacks = []\n",
        "\n",
        "    if cfg.checkpoint.enabled:\n",
        "        # save checkpoint on last epoch only\n",
        "        ckpt = Checkpointer(\n",
        "            cfg,\n",
        "            logdir=os.path.join(cfg.checkpoint.dir, \"linear\"),\n",
        "            frequency=cfg.checkpoint.frequency,\n",
        "            keep_prev=cfg.checkpoint.keep_prev,\n",
        "        )\n",
        "        callbacks.append(ckpt)\n",
        "\n",
        "    # wandb logging\n",
        "    if cfg.wandb.enabled:\n",
        "        wandb_logger = WandbLogger(\n",
        "            name=cfg.name,\n",
        "            project=cfg.wandb.project,\n",
        "            entity=cfg.wandb.entity,\n",
        "            offline=cfg.wandb.offline,\n",
        "            resume=\"allow\" if wandb_run_id else None,\n",
        "            id=wandb_run_id,\n",
        "        )\n",
        "        wandb_logger.watch(model, log=\"gradients\", log_freq=100)\n",
        "        wandb_logger.log_hyperparams(OmegaConf.to_container(cfg))\n",
        "\n",
        "        # lr logging\n",
        "        lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
        "        callbacks.append(lr_monitor)\n",
        "\n",
        "    trainer_kwargs = OmegaConf.to_container(cfg)\n",
        "    # we only want to pass in valid Trainer args, the rest may be user specific\n",
        "    valid_kwargs = inspect.signature(Trainer.__init__).parameters\n",
        "    trainer_kwargs = {name: trainer_kwargs[name] for name in valid_kwargs if name in trainer_kwargs}\n",
        "    trainer_kwargs.update(\n",
        "        {\n",
        "            \"logger\": wandb_logger if cfg.wandb.enabled else None,\n",
        "            \"callbacks\": callbacks,\n",
        "            \"enable_checkpointing\": False,\n",
        "            \"strategy\": DDPStrategy(find_unused_parameters=False)\n",
        "            if cfg.strategy == \"ddp\"\n",
        "            else cfg.strategy,\n",
        "        }\n",
        "    )\n",
        "    trainer = Trainer(**trainer_kwargs)\n",
        "\n",
        "    # fix for incompatibility with nvidia-dali and pytorch lightning\n",
        "    # with dali 1.15 (this will be fixed on 1.16)\n",
        "    # https://github.com/Lightning-AI/lightning/issues/12956\n",
        "    try:\n",
        "        from pytorch_lightning.loops import FitLoop\n",
        "\n",
        "        class WorkaroundFitLoop(FitLoop):\n",
        "            @property\n",
        "            def prefetch_batches(self) -> int:\n",
        "                return 1\n",
        "\n",
        "        trainer.fit_loop = WorkaroundFitLoop(\n",
        "            trainer.fit_loop.min_epochs, trainer.fit_loop.max_epochs\n",
        "        )\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    if cfg.data.format == \"dali\":\n",
        "        trainer.fit(model, ckpt_path=ckpt_path, datamodule=dali_datamodule)\n",
        "    else:\n",
        "        trainer.fit(model, train_loader, val_loader, ckpt_path=ckpt_path)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}