{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPK/epw1L8rB4+tXn4KwlW1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NhanDuy2003/solo-learn/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duq4T1b-pEPX"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "import os\n",
        "\n",
        "import hydra\n",
        "import torch\n",
        "from omegaconf import DictConfig, OmegaConf\n",
        "from pytorch_lightning import Trainer, seed_everything\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.strategies.ddp import DDPStrategy\n",
        "\n",
        "from solo.args.pretrain import parse_cfg\n",
        "from solo.data.classification_dataloader import prepare_data as prepare_data_classification\n",
        "from solo.data.pretrain_dataloader import (\n",
        "    FullTransformPipeline,\n",
        "    NCropAugmentation,\n",
        "    build_transform_pipeline,\n",
        "    prepare_dataloader,\n",
        "    prepare_datasets,\n",
        ")\n",
        "from solo.methods import METHODS\n",
        "from solo.utils.auto_resumer import AutoResumer\n",
        "from solo.utils.checkpointer import Checkpointer\n",
        "from solo.utils.misc import make_contiguous, omegaconf_select\n",
        "\n",
        "try:\n",
        "    from solo.data.dali_dataloader import PretrainDALIDataModule, build_transform_pipeline_dali\n",
        "except ImportError:\n",
        "    _dali_avaliable = False\n",
        "else:\n",
        "    _dali_avaliable = True\n",
        "\n",
        "try:\n",
        "    from solo.utils.auto_umap import AutoUMAP\n",
        "except ImportError:\n",
        "    _umap_available = False\n",
        "else:\n",
        "    _umap_available = True\n",
        "\n",
        "\n",
        "@hydra.main(version_base=\"1.2\")\n",
        "def main(cfg: DictConfig):\n",
        "    # hydra doesn't allow us to add new keys for \"safety\"\n",
        "    # set_struct(..., False) disables this behavior and allows us to add more parameters\n",
        "    # without making the user specify every single thing about the model\n",
        "    OmegaConf.set_struct(cfg, False)\n",
        "    cfg = parse_cfg(cfg)\n",
        "\n",
        "    seed_everything(cfg.seed)\n",
        "\n",
        "    assert cfg.method in METHODS, f\"Choose from {METHODS.keys()}\"\n",
        "\n",
        "    if cfg.data.num_large_crops != 2:\n",
        "        assert cfg.method in [\"wmse\", \"mae\"]\n",
        "\n",
        "    model = METHODS[cfg.method](cfg)\n",
        "    make_contiguous(model)\n",
        "    # can provide up to ~20% speed up\n",
        "    if not cfg.performance.disable_channel_last:\n",
        "        model = model.to(memory_format=torch.channels_last)\n",
        "\n",
        "    # validation dataloader for when it is available\n",
        "    if cfg.data.dataset == \"custom\" and (cfg.data.no_labels or cfg.data.val_path is None):\n",
        "        val_loader = None\n",
        "    elif cfg.data.dataset in [\"imagenet100\", \"imagenet\"] and cfg.data.val_path is None:\n",
        "        val_loader = None\n",
        "    else:\n",
        "        if cfg.data.format == \"dali\":\n",
        "            val_data_format = \"image_folder\"\n",
        "        else:\n",
        "            val_data_format = cfg.data.format\n",
        "\n",
        "        _, val_loader = prepare_data_classification(\n",
        "            cfg.data.dataset,\n",
        "            train_data_path=cfg.data.train_path,\n",
        "            val_data_path=cfg.data.val_path,\n",
        "            data_format=val_data_format,\n",
        "            batch_size=cfg.optimizer.batch_size,\n",
        "            num_workers=cfg.data.num_workers,\n",
        "        )\n",
        "\n",
        "    # pretrain dataloader\n",
        "    if cfg.data.format == \"dali\":\n",
        "        assert (\n",
        "            _dali_avaliable\n",
        "        ), \"Dali is not currently avaiable, please install it first with pip3 install .[dali].\"\n",
        "        pipelines = []\n",
        "        for aug_cfg in cfg.augmentations:\n",
        "            pipelines.append(\n",
        "                NCropAugmentation(\n",
        "                    build_transform_pipeline_dali(\n",
        "                        cfg.data.dataset, aug_cfg, dali_device=cfg.dali.device\n",
        "                    ),\n",
        "                    aug_cfg.num_crops,\n",
        "                )\n",
        "            )\n",
        "        transform = FullTransformPipeline(pipelines)\n",
        "\n",
        "        dali_datamodule = PretrainDALIDataModule(\n",
        "            dataset=cfg.data.dataset,\n",
        "            train_data_path=cfg.data.train_path,\n",
        "            transforms=transform,\n",
        "            num_large_crops=cfg.data.num_large_crops,\n",
        "            num_small_crops=cfg.data.num_small_crops,\n",
        "            num_workers=cfg.data.num_workers,\n",
        "            batch_size=cfg.optimizer.batch_size,\n",
        "            no_labels=cfg.data.no_labels,\n",
        "            data_fraction=cfg.data.fraction,\n",
        "            dali_device=cfg.dali.device,\n",
        "            encode_indexes_into_labels=cfg.dali.encode_indexes_into_labels,\n",
        "        )\n",
        "        dali_datamodule.val_dataloader = lambda: val_loader\n",
        "    else:\n",
        "        pipelines = []\n",
        "        for aug_cfg in cfg.augmentations:\n",
        "            pipelines.append(\n",
        "                NCropAugmentation(\n",
        "                    build_transform_pipeline(cfg.data.dataset, aug_cfg), aug_cfg.num_crops\n",
        "                )\n",
        "            )\n",
        "        transform = FullTransformPipeline(pipelines)\n",
        "\n",
        "        if cfg.debug_augmentations:\n",
        "            print(\"Transforms:\")\n",
        "            print(transform)\n",
        "\n",
        "        train_dataset = prepare_datasets(\n",
        "            cfg.data.dataset,\n",
        "            transform,\n",
        "            train_data_path=cfg.data.train_path,\n",
        "            data_format=cfg.data.format,\n",
        "            no_labels=cfg.data.no_labels,\n",
        "            data_fraction=cfg.data.fraction,\n",
        "        )\n",
        "        train_loader = prepare_dataloader(\n",
        "            train_dataset, batch_size=cfg.optimizer.batch_size, num_workers=cfg.data.num_workers\n",
        "        )\n",
        "\n",
        "    # 1.7 will deprecate resume_from_checkpoint, but for the moment\n",
        "    # the argument is the same, but we need to pass it as ckpt_path to trainer.fit\n",
        "    ckpt_path, wandb_run_id = None, None\n",
        "    if cfg.auto_resume.enabled and cfg.resume_from_checkpoint is None:\n",
        "        auto_resumer = AutoResumer(\n",
        "            checkpoint_dir=os.path.join(cfg.checkpoint.dir, cfg.method),\n",
        "            max_hours=cfg.auto_resume.max_hours,\n",
        "        )\n",
        "        resume_from_checkpoint, wandb_run_id = auto_resumer.find_checkpoint(cfg)\n",
        "        if resume_from_checkpoint is not None:\n",
        "            print(\n",
        "                \"Resuming from previous checkpoint that matches specifications:\",\n",
        "                f\"'{resume_from_checkpoint}'\",\n",
        "            )\n",
        "            ckpt_path = resume_from_checkpoint\n",
        "    elif cfg.resume_from_checkpoint is not None:\n",
        "        ckpt_path = cfg.resume_from_checkpoint\n",
        "        del cfg.resume_from_checkpoint\n",
        "\n",
        "    callbacks = []\n",
        "\n",
        "    if cfg.checkpoint.enabled:\n",
        "        # save checkpoint on last epoch only\n",
        "        ckpt = Checkpointer(\n",
        "            cfg,\n",
        "            logdir=os.path.join(cfg.checkpoint.dir, cfg.method),\n",
        "            frequency=cfg.checkpoint.frequency,\n",
        "            keep_prev=cfg.checkpoint.keep_prev,\n",
        "        )\n",
        "        callbacks.append(ckpt)\n",
        "\n",
        "    if omegaconf_select(cfg, \"auto_umap.enabled\", False):\n",
        "        assert (\n",
        "            _umap_available\n",
        "        ), \"UMAP is not currently avaiable, please install it first with [umap].\"\n",
        "        auto_umap = AutoUMAP(\n",
        "            cfg.name,\n",
        "            logdir=os.path.join(cfg.auto_umap.dir, cfg.method),\n",
        "            frequency=cfg.auto_umap.frequency,\n",
        "        )\n",
        "        callbacks.append(auto_umap)\n",
        "\n",
        "    # wandb logging\n",
        "    if cfg.wandb.enabled:\n",
        "        wandb_logger = WandbLogger(\n",
        "            name=cfg.name,\n",
        "            project=cfg.wandb.project,\n",
        "            entity=cfg.wandb.entity,\n",
        "            offline=cfg.wandb.offline,\n",
        "            resume=\"allow\" if wandb_run_id else None,\n",
        "            id=wandb_run_id,\n",
        "        )\n",
        "        wandb_logger.watch(model, log=\"gradients\", log_freq=100)\n",
        "        wandb_logger.log_hyperparams(OmegaConf.to_container(cfg))\n",
        "\n",
        "        # lr logging\n",
        "        lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
        "        callbacks.append(lr_monitor)\n",
        "\n",
        "    trainer_kwargs = OmegaConf.to_container(cfg)\n",
        "    # we only want to pass in valid Trainer args, the rest may be user specific\n",
        "    valid_kwargs = inspect.signature(Trainer.__init__).parameters\n",
        "    trainer_kwargs = {name: trainer_kwargs[name] for name in valid_kwargs if name in trainer_kwargs}\n",
        "    trainer_kwargs.update(\n",
        "        {\n",
        "            \"logger\": wandb_logger if cfg.wandb.enabled else None,\n",
        "            \"callbacks\": callbacks,\n",
        "            \"enable_checkpointing\": False,\n",
        "            \"strategy\": DDPStrategy(find_unused_parameters=False)\n",
        "            if cfg.strategy == \"ddp\"\n",
        "            else cfg.strategy,\n",
        "        }\n",
        "    )\n",
        "    trainer = Trainer(**trainer_kwargs)\n",
        "\n",
        "    # fix for incompatibility with nvidia-dali and pytorch lightning\n",
        "    # with dali 1.15 (this will be fixed on 1.16)\n",
        "    # https://github.com/Lightning-AI/lightning/issues/12956\n",
        "    try:\n",
        "        from pytorch_lightning.loops import FitLoop\n",
        "\n",
        "        class WorkaroundFitLoop(FitLoop):\n",
        "            @property\n",
        "            def prefetch_batches(self) -> int:\n",
        "                return 1\n",
        "\n",
        "        trainer.fit_loop = WorkaroundFitLoop(\n",
        "            trainer.fit_loop.min_epochs, trainer.fit_loop.max_epochs\n",
        "        )\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    if cfg.data.format == \"dali\":\n",
        "        trainer.fit(model, ckpt_path=ckpt_path, datamodule=dali_datamodule)\n",
        "    else:\n",
        "        trainer.fit(model, train_loader, val_loader, ckpt_path=ckpt_path)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}